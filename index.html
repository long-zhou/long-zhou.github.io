<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="author" content="Long Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="/">

    <title>Long Zhou's homepage</title>

    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Long Zhou's Blog" href="rss.xml" />
    <link href="static/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Google Analytics -->
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-37166920-1']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>

    <style>
      body {
        font-size: 16px;
        padding-bottom: 20px;
      }

      h1 { font-size: 30px; line-height: 135%;}
      h2 { font-size: 24px; line-height: 135%;}
      h3 { font-size: 20px; line-height: 135%;}
      h4 { font-size: 18px; }
      h5 { font-size: 16px; }
      h6 { font-size: 13px; }

      hr {
        border-top: 2px solid #e5e5e5;
      }

      p {
        text-align: justify;
      }

      .navbar {
        margin-bottom: 5px;
      }
    </style>
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-static-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
          </button>
          <span class="navbar-brand">Long Zhou</span>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li  class="active"  >
              <a href="index.html">Home</a>
            </li>
            
            <li  class="active"  >
              <a href="publications.html">Publications</a>
            </li>
            <!--
            <li  class="active"  >
              <a href="blog.html">Blog</a>
            </li>
            -->
          </ul>
        </div>
      </div>
    </nav>

    <div class="container">

<style>
  li.paper {
    margin-left: 20px;
    margin-bottom: 10px;
    font-size: 15px;
  }

  li.paper .title {
    font-size: 3;
  }

  li.paper .me {
    background-color: #eeffee;
  }
</style>


<div class="row">
  <div class="col-sm-3 col-md-2">
    <img src="static/lz.jpg" class="img-thumbnail" width="320px"
    style="margin-top: 10px; margin-bottom: 15px">
  </div>
  <div class="col-sm-9 col-md-10">
    <h1>Long Zhou (周 龙)</h1>
    <p>
      <a href="https://www.microsoft.com/en-us/research/people/lozhou/">Senior Researcher</a><br>
      <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a><br>
      Email: wszlong@gmail.com</p>
    </p>
  </div>
</div>

<p>
  <!--
<img class="eq" src="static/8192024641-130.png" alt="@" style="vertical-align: -1px">
  Hello! I am a Phd student at National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), working on 
   supervised by Prof. <a href="http://www.nlpr.ia.ac.cn/cip/cqzong.htm">Chengqing Zong</a>. My research interests are natural language processing and machine translation.
   -->
   <!-- working with <a href="https://www.microsoft.com/en-us/research/people/shujliu/">Shujie Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/fuwei/">Furu Wei</a>, and <a href="https://scholar.google.co.jp/citations?user=a0w5c0gAAAAJ&hl=en">Ming Zhou</a> -->

  Hello! I am a senior researcher in <a href="https://www.microsoft.com/en-us/research/group/natural-language-computing/">Natural Language Computing group</a> at Microsoft Research Asia, working with Dr. <a href="https://www.microsoft.com/en-us/research/people/shujliu/">Shujie Liu</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/fuwei/">Furu Wei</a>, and Dr. <a href="https://scholar.google.co.jp/citations?user=a0w5c0gAAAAJ&hl=en">Ming Zhou</a>. 
  Before joining MSRA, I received my Ph.D. in 2020 from <a href="http://www.nlpr.ia.ac.cn/en/">NLPR</a> of CASIA under the supervision of Prof. <a href="http://www.nlpr.ia.ac.cn/cip/english/zong.htm">Chengqing Zong</a>, working with Prof. <a href="https://nlpr.ia.ac.cn/cip/jjzhang.htm">Jiajun Zhang</a>.
  My research interests include natural/spoken/programming language processing and multi-modal large language models.
</p>



<hr>
<h2>News</h2> 

<p></p>

<ul>

  <!--
<li type="circle"><font size="3" color="red">I am looking for a job in natural language processing and machine translation. Here is my 
       <a href="static/pub/CV-LongZhou-2019.pdf">[resume]</a>.
</font></li>
   <li> <a href="https://www.weibo.com/2352674743/profile?rightmod=1&wvr=6&mod=personinfo">Weibo</a> </li>

   <li type="circle"><font size="3" color="black">I have one paper (A Compact and Language-Sensitive Multilingual Translation Method) accepted by ACL 2019.
</font></li>
  <li type="circle"><font size="3" color="black">I have one paper (Sequence Generation: From Both Sides to the Middle) accepted by IJCAI 2019.
</font></li>
  <li type="circle"><font size="3" color="black">I have one paper (Synchronous Bidirectional Neural Machine Translation) accepted by Transactions of ACL 2019.
</font></li>
  <li type="circle"><font size="3" color="black">We wrote a comprehensive survey paper on <a href="https://arxiv.org/abs/2203.14101">A Roadmap for Big Model</a>.
</font></li>
-->
  <li type="circle"><font size="3" color="black">Language modeling for speech synthesis families: <a href="https://arxiv.org/abs/2301.02111">VALL-E</a>, <a href="https://arxiv.org/abs/2303.03926">VALL-E X</a>, <a href="https://arxiv.org/abs/2406.07855">VALL-E R</a>, <a href="https://arxiv.org/abs/2406.05370">VALL-E 2</a>, <a href="https://arxiv.org/abs/2305.16107">VioLA</a>, and <a href="https://arxiv.org/abs/2407.08551">MELLE</a>.
</font></li>
  <li type="circle"><font size="3" color="black">Cross-lingual codec language model <a href="https://arxiv.org/abs/2303.03926">VALL-E X</a> [<a href="https://aka.ms/vallex">Demo</a>]: Enable you to speak foreign languages with your own voice.
  </font></li>
    <li type="circle"><font size="3" color="black">02/2023: Give a talk at <a href="https://mp.weixin.qq.com/s/mmt9xuk3SBwTGrX3U0zsTw">Research Lecture Series of Microsoft Research Asia</a> on "<a href="static/pub/Unified-Speech-and-Text-Pre-Training.pdf">Unified Speech and Text Pre-Training</a>". 
</font></li>
    <li type="circle"><font size="3" color="black">We proposed two joint speech-text pre-trained models: <a href="https://arxiv.org/abs/2209.15329">SpeechLM</a> (encoder) and <a href="https://arxiv.org/abs/2210.03730">SpeechUT</a> (encoder-decoder).
</font></li>
    <li type="circle"><font size="3" color="black">The end-to-end speech translation system <a href="https://arxiv.org/abs/2206.05777">YiTrans</a> achieved top results on <a href="https://iwslt.org/2022/">IWSLT 2022</a> shared tasks.
</font></li>
    <li type="circle"><font size="3" color="black">We proposed two pre-trained speech models <a href="https://arxiv.org/abs/2110.07205">SpeechT5</a> and <a href="https://arxiv.org/abs/2110.13900">WavLM</a> for spoken language processing tasks.
</font></li>
    <li type="circle"><font size="3" color="black">10/10/2020: Give a talk at <a href="http://sc.cipsc.org.cn/mt/conference/2020/">CCMT 2020</a> tutorial on "<a href="static/pub/CCMT2020-Tutorial-LongZhou.pdf">Revisiting Text Generation Methods in Neural Machine Translation</a>". 
</font></li>
    <li type="circle"><font size="3" color="black">We released <a href="https://www.microsoft.com/en-us/research/blog/codexglue-a-benchmark-dataset-and-open-challenge-for-code-intelligence/">CodeXGLUE</a> (<a href="https://www.msra.cn/zh-cn/news/features/codexglue">Chinese link</a>), 
    a benchmark dataset and open challenge for code intelligence.
</font></li>
    <li type="circle"><font size="3" color="black">We introduced a pre-trained model <a href="https://arxiv.org/abs/2009.08366">GraphCodeBERT</a> and an evulation metric <a href="https://arxiv.org/abs/2009.10297">CodeBLEU</a> for program languages.
</font></li>

<!--
    <li type="circle"><font size="3" color="black">I joined the <a href="https://www.microsoft.com/en-us/research/group/natural-language-computing/">Natural Language Computing group</a> at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> as a researcher in july 2020.
</font></li>  
-->
  
</ul>

<hr>
<h2> Selected Publications  </h2> 

<h4> <b>&nbsp;&nbsp;&nbsp;&nbsp;Spoken Language Processing</b> </h4> 

<ul>

  <li class="paper"> 
    <a href="https://arxiv.org/abs/2110.07205"> <u>SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</u></a>. Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. ACL 2022.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2110.13900"> <u>WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</u></a>. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei. IEEE Journal of Selected Topics in Signal Processing, 2022.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2210.03730"> <u>SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training</u></a>. Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu, Lirong Dai, Jinyu Li, Furu Wei. EMNLP 2022.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2209.15329"> <u>SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data</u></a>. Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, Furu Wei. IEEE/ACM Transactions on Audio Speech and Language Processing.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2211.11275"> <u>VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning</u></a>. Qiushi Zhu, Long Zhou, Ziqiang Zhang, Shujie Liu, Binxing Jiao, Jie Zhang, Lirong Dai, Daxin Jiang, Jinyu Li, Furu Wei. IEEE Transactions on Multimedia.<br>
  </li>
  <li class="paper"> 
    <a href="https://arxiv.org/abs/2107.05876"> <u>A Configurable Multilingual Model is All You Need to Recognize All Languages</u></a>. Long Zhou, Jinyu Li, Eric Sun, Shujie Liu. ICASSP 2022.<br>
  </li>
  <li class="paper"> 
  <a href="https://arxiv.org/abs/2303.03926"> <u>Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</u></a>. Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei. Arxiv 2023.<br>
  </li>

<!--
  <li class="paper"> 
    <a href="https://arxiv.org/abs/2110.05036"> <u>Multi-View Self-Attention Based Transformer for Speaker Recognition</u></a>. Rui Wang, Junyi Ao, Long Zhou, Shujie Liu, Zhihua Wei, Tom Ko, Qing Li, Yu Zhang. ICASSP 2022.<br>
  </li>
  <li class="paper">
    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6360"> <u>Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding</u></a>. Yuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, Hua Wu, Haifeng Wang, Chengqing Zong. AAAI 2020.<br>
  </li>
-->

</ul>


<h4> <b>&nbsp;&nbsp;&nbsp;&nbsp;Programming Language Processing</b> </h4> 

<ul>  
  <li class="paper">
    <a href="https://arxiv.org/abs/2009.08366"> <u>GraphCodeBERT: Pre-training Code Representations with Data Flow</u></a>. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou. ICLR 2021.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2102.04664"> <u>CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</u></a>. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu. NeurIPS 2021 Datasets and Benchmarks Track.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2009.10297"> <u>CodeBLEU: A Method for Automatic Evaluation of Code Synthesis</u></a>. Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, Shuai Ma. Arxiv 2020.<br>
  </li>
  <li class="paper">
    <a href="https://arxiv.org/abs/2109.12296"> <u>Jointly Learning to Repair Code and Generate Commit Message</u></a>. Jiaqi Bai, Long Zhou, Ambrosio Blanco, Shujie Liu, Furu Wei, Ming Zhou, Zhoujun Li. EMNLP 2021.<br>
  </li>
  <li class="paper">
    <a href="https://aclanthology.org/2021.findings-acl.111.pdf"> <u>Grammar-Based Patches Generation for Automated Program Repair</u></a>. Yu Tang, Long Zhou, Ambrosio Blanco, Shujie Liu, Furu Wei, Ming Zhou, Muyun Yang. Findings of ACL 2021.<br>
  </li>

</ul>


<h4> <b>&nbsp;&nbsp;&nbsp;&nbsp;Natural Language Processing</b> </h4> 

<ul>
  <li class="paper">
    <a href="https://arxiv.org/abs/1704.06393"> <u>Nueral System Combination for Machine Translation</u></a>. Long Zhou, Wenpeng Hu, Jiajun Zhang, Chengqing Zong. ACL 2017.<br>
  </li>
  <li class="paper">
    <a href="https://link.springer.com/chapter/10.1007/978-3-319-73618-1_18"> <u>Look-ahead Attention for Generation in Neural Machine Translation</u></a>. Long Zhou, Jiajun Zhang, Chengqing Zong. NLPCC 2017 (<font size="3" color="red">Best Paper Award</font>).<br>
  </li>
  <li class="paper">
    <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00256/43504/Synchronous-Bidirectional-Neural-Machine"> <u>Synchronous Bidirectional Neural Machine Translation</u></a>. Long Zhou, Jiajun Zhang, Chengqing Zong. TACL 2019.<br>
  </li> 
  <li class="paper">
    <a href="https://arxiv.org/abs/1906.09601"> <u>Sequence Generation: From Both Sides to the Middle</u></a>. Long Zhou, Jiajun Zhang, Chengqing Zong, Heng Yu. IJCAI 2019.<br>
  </li> 
  <li class="paper">
    <a href="https://aclanthology.org/P19-1117/"> <u>A Compact and Language-Sensitive Multilingual Translation Method</u></a>. Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai, Jingfang Xu, Chengqing Zong. ACL 2019.<br>
  </li> 
  <li class="paper">
    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0004370220300011"> <u>Synchronous Bidirectional Inference for Neural Sequence Generation</u></a>. Jiajun Zhang, Long Zhou, Yang Zhao, Chengqing Zong. Journal of Artificial Intelligence, 2020.<br>
  </li> 
  <li class="paper">
    <a href="https://aclanthology.org/2021.acl-long.348/"> <u>SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation</u></a>. Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming Zhou, Shuai Ma. ACL 2021.<br>
  </li> 
  
</ul>


<hr>
<h2>Software</h2>

<ul>
  <li><p><a href="https://github.com/microsoft/SpeechT5">SpeechLM: </a> Unified-Modal Speech-Text Pre-Training for Spoken Language Processing.</p>
  </li>
</ul>

<ul>
  <li><p><a href="https://github.com/microsoft/unilm">unilm: </a> Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.</p>
  </li>
</ul>
    
<ul>
<li><p><a href="https://github.com/wszlong/transformer">Transformer: </a> A simple Tensorflow implementation of the Transformer.</p>
</li>
</ul>
    
<ul>
<li><p><a href="https://github.com/wszlong/transformer-translator">Transformer-translator: </a> A C++/CUDA toolkit for Transformer (NMT) Translator (Decoder).</p>
</li>
</ul>
    
<ul>
<li><p><a href="https://github.com/wszlong/sb-nmt">SB-NMT: </a> Code for Synchronous Bidirectional Neural Machine Translation (SB-NMT).</p>
</li>
</ul>

<ul>
<li><p><a href="https://github.com/wszlong/rnmt">RNMT: </a> A C++/CUDA toolkit for neural machine translation (RNN-Based NMT) across multiple GPUs.</p>
</li>
</ul>

<hr>


<!-- <h2>Competition</h2>
    
<ul>
<li><p> CoNLL-2016 Shared Task on Chinese Shallow Discourse Parsing, <b>First Place</b>.</p>
</li>
</ul>
    
<ul>
<li><p> Machine Translation Evaluation in CWMT 2017, <b>First Place & Third Place</b>. </p>
</li>
</ul>
    
<ul>
<li><p> The English-Chinese Machine Translation track in AI Challenger 2017, <b>Third Place</b>. </p>
</li>
</ul>

<ul>
<li><p> Minority Language Machine Translation Evaluation in CCMT 2019, <b>Second Place</b>. </p>
</li>
</ul>
    
 <ul>
<li><p> The Chinese<->Japanese Machine Translation task in IWSLT 2020, <b>First Place</b>. </p>
</li>
</ul> -->

<hr>    
<h2>Awards</h2>

<ul>
  <li> VALL-E wins the <a href="https://netexplo.com/n100/">UNESCO Netexplo Innovation Award 2023</a>, 2023</li>
  <li> Nomination Award for Excellent Doctoral Thesis of Chinese Information Processing Society, 2020</li>
  <li> CAS Presidential Scholarship, 2020 </li>
  <li> NVIDIA Scholarship, 2018 </li>
  <li> NLPCC2017 Best Paper Award, 2017 </li>
  <li> Outstanding University Graduates of Sichuan Province, 2015 (Top 1%) </li>
  <li> National Scholarship, 2012/2013/2014 (Top 2%) </li>
</ul>


<hr>    
<h2>Links</h2>

<ul>
  <li> <a href="https://github.com/wszlong">Github</a> </li>
  <li> <a href="https://www.linkedin.com/in/long-zhou-262200194/">Linkedin</a> </li>
  <li> <a href="https://scholar.google.com/citations?user=ZnwgSXIAAAAJ&hl=en">Google Scholar</a> </li>
  <li> <a href="https://www.microsoft.com/en-us/research/people/lozhou/">Microsoft Profile</a> </li>
</ul>

  <hr>

    </div>
    <script src="//code.jquery.com/jquery.js"></script>
    <script src="static/bootstrap/js/bootstrap.js"></script>
    <small>
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=bo1ze3BPOfkTsQ_4-5F-jFe5yvno79OD6V8hzacGhL4'></script>
    <small>
    </body>
</html>
